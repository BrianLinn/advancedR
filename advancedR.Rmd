---
title: "Advanced R Notebook"
output: html_notebook
---

Dejan Sarka - presenter

#Introduction to Data Science
reinforcement learning

https://goo.gl/K9YovT

What is Data Science? Extracting knowledge from data - statistics and programming
Statistics is a science - data mining and machine learning are more business oriented

Statistics  small <30, medium, >200 < 100K) Large > 100K
DM - medium
ML - medium and large

##Vectors
ordered collections of numbers
created with c() to concatenate elements or sub-vectors
rep() to repeat elements or patterns
seq() or m:n to generate sequences

functions can be applied to vectors without loops
use the [] operator to select elements

##Packages
packages are stored in library
installed.package = installed packages
library() list packages in library


```{r basic expressions and variables}

1+1
2 + 3 * 4
3 ^ 3
sqrt(81)
pi

#Check built-in constants
??"constants"

#Sequences
rep(1, 5)
seq(4, 8)
seq(4, 20, by = 3)

#Assignment
x <- 2
y <- 3
z <- 4
X + y + z #error as variables are case sensitive
x + y + z

#Vectors
x <- c(2, 0, 0, 4); x
assign("y", c(1, 9, 9, 9)); y
c(5, 4, 3, 2) >- z; z
q = c(1, 2, 3, 4); q

installed.packages()
.libPaths()
install.packages("RODBC")
library(RODBC)

# # Ad-hoc connection
# con <- odbcDriverConnect('driver={SQL Server};server=SQL2017EIM;
#                          database=AdventureWorksDW2017;uid=RUser;pwd=Pa$$w0rd')
# 
# # DSN connection
# con <- odbcConnect("AWDW", uid = "RUser", pwd = "Pa$$w0rd")
# 
# # Read SQL Server data
# sqlQuery(con,
#          "SELECT CustomerKey,
#          EnglishEducation AS Education,
#          Age, NumberCarsOwned, BikeBuyer
#          FROM dbo.vTargetMail;")
# close(con)
```

##Collections and Objects
###Matrices (Arrays) 
  -multidimensional generalizations of vectors
  
###Factors 
  -compact categorical data handling
  -Can have discrete or coninuous values
  -Discrete values can be nominal or catgorical when labeling, or ordinal if there is intrinsic order to data
  -Levels represent distinct values - order must be defined
  
###Lists 
  - ordered collection of different data structures
  
###Data Frames or matrix-like structures
  

-Functions


```{r}
# Matrix
x = c(1, 2, 3, 4, 5, 6); x
Y = array(x, dim = c(2, 3)); Y
Z = matrix(x, 2, 3, byrow = F); Z
U = matrix(x, 2, 3, byrow = T); U

# Using explicit names
rnames = c("rrr1", "rrr2")
cnames = c("ccc1", "ccc2", "ccc3")
V = matrix(x, 2, 3, byrow = T,
           dimnames = list(rnames, cnames))
V

# Array
rnames = c("rrr1", "rrr2")
cnames = c("ccc1", "ccc2", "ccc3")
pnames = c("ppp1", "ppp2", "ppp3")
Y = array(1:18, dim = c(2, 3, 3),
          dimnames = list(rnames, cnames, pnames))
Y

# Factor
x = c("good", "moderate", "good", "bad", "bad", "good"); x
y = factor(x); y
z = factor(x, order = TRUE); z
w = factor(x, order = TRUE,
           levels = c("bad", "moderate", "good")); w

# List
L = list(name1 = "ABC", name2 = "DEF",
         no.children = 2, children.ages = c(3, 6))
L
L[[1]]
L[[4]]
L[[4]][2]

# Data frame
CategoryId = c(1, 2, 3, 4)
CategoryName = c("Bikes", "Components", "Clothing", "Accessories")
ProductCategories = data.frame(CategoryId, CategoryName)
ProductCategories

library(RODBC)
# Reading in a data frame from SQL Server
# con <- odbcConnect("AWDW", uid = "RUser", pwd = "Pa$$w0rd")
# TM <-
#   sqlQuery(con,
#            "SELECT CustomerKey,
#             EnglishEducation AS Education,
#             Age, NumberCarsOwned, BikeBuyer
#           FROM dbo.vTargetMail;")
# close(con)
# head(TM, 5)
# tail(TM, 5)
# TM[1:5, 1:5]
# TM[1:5, c("CustomerKey", "BikeBuyer")]
# TM[TM$CustomerKey >= 29479, c("CustomerKey", "BikeBuyer")]

# # Check the complete data frame
# View(TM)
# 
# # Crosstabulation of BikeBuyer and NumberCarsOwned
# table(TM$NumberCarsOwned, TM$BikeBuyer)
# 
# # Data table
# # install.packages("data.table")
# library(data.table)
# 
# dtTM <- data.table(TM)
# class(dtTM)
# # See explicit row numbers
# dtTM
# 
# # View works
# View(dtTM)
# 
# # Aggregation
# dtTM[, mean(Age)]
# # The following line does not work
# TM[, mean(Age)]
# 
# # More aggregations
# dtTM[, .N]   # counts
# dtTM[, .(Count=.N), by = NumberCarsOwned]
# dtTM[, .(AverageAge=mean(Age)), by = NumberCarsOwned]
# 
# # Filter
# dtTM[NumberCarsOwned > 1, .(AverageAge=mean(Age)),
#      by = NumberCarsOwned]
# 
# # Order
# head(dtTM[order(-CustomerKey)], 5)

```


#Data Preparation and Overview

##Frequencies and Dummies
-Frequency tables and graphs are basic representations of discrete variables
  Value, absolute frequenct, percentage, cumulative stats, histograms, etc...
  
-Order ordinals correctly

-When numerical values needed - change nominals to discrete
  called dummies
  Typically 1 and 0
  
##Population Moments  

###Centers of distribution
  median is the value at half the cases
  mean is the average
  median compared to mean can indicate direction of distribution

###Spread
  interquartile range (IQR)
  upper and lower quartile
  standard deviation
  compare sd and quartile to determine how wide or narrow the data is distributed

###Higher poulation moments
  skewness
  kurtosis

sd/mean = coefficient of variation

```{r}
library(tidyverse)
# Anscombe data set
data("anscombe")
View(anscombe)
attach(anscombe)

# Correlations
cor(x1, y1)
cor(x2, y2)
cor(x3, y3)
cor(x4, y4)
# All 0.8165214

# View the data

# Defining a 2x2 graph
oldpar <- par(no.readonly = TRUE)
par(mfrow = c(2, 2))

plot(x1, y1, main = '1')
plot(x2, y2, main = '2')
plot(x3, y3, main = '3')
plot(x4, y4, main = '4')

# Clean up
par(oldpar)
detach(anscombe)


#Always check the graphs - hostograms, bar charts, scatterplots

```

##Missing Values
* Empty, nonexistent, uncollected data

* Before handling, determine if there are any patterns in the missing data

* Handling
+ do nothing
+ filter missing data
  + ignore column
  + predict new values
  + build separate models
  + modify systems to gather missing data
  + replace with common (mean) values


```{r}
TM <- read_csv("C:/MOVEDELETE/OneDrive-2018-11-05/AdvancedRCode/TM.csv")
# Get dummies in R
#install.packages("dummies")
library(dummies)
# 
# # Create the dummies
 TM1 <- cbind(TM, dummy(TM$Education, sep = "_"))
# # Last 10 rows
 tail(TM1, 10)
# 
# # Descriptive statistics
 attach(TM);
# # A quick summary for the whole dataset
 summary(TM);
# 
# # A quick summary for Age
 summary(Age);
# # Details for Age
 mean(Age);
 median(Age);
 min(Age);
 max(Age);
 range(Age);
 quantile(Age, 1/4);
 quantile(Age, 3/4);
 IQR(Age);
 var(Age);
 sd(Age);

# Custom function for skewness and kurtosis
 skewkurt <- function(p){
   avg <- mean(p)
   cnt <- length(p)
   stdev <- sd(p)
   skew <- sum((p-avg)^3/stdev^3)/cnt
   kurt <- sum((p-avg)^4/stdev^4)/cnt-3
   return(c(skewness=skew, kurtosis=kurt))
 };
 skewkurt(Age);
```

###Outliers

* Rare and far out-of-bound values
* Before handling, determine if a pattern exists
* Approaches
    + check if outlier is accurate or erroneous
    + Do nothing
    + Filter outlier rows
    + Ignore column
    + Replace outliers with common (mean) values
    + Bin values into equal height ranges
    + Normalize data values
  
###Smoothing
####Moving Averages

* Simple moving average
* Weighted moving average
* Exponential moving average

###Data Values Normalization

* Normalize values in the same scale
    + Range
    + z-score
    + hyperbolic tangent function
  
###Derived variables

* Simple facts are not enough - how do you measure churn
*  Most of the time there are hunches
    + extract features from attributes


###combinations of columns
Height^2/Weight = Obesity Index
Passengers * Miles

###Package dplyr

* Functions for data manipulation
    + select and rename
    + filter
    + arrange
    + etc...
  
  
##Associations

Chi-Squared formula:
• For the Chi-Squared distribution there are
already prepared tables with critical points
for different degrees of freedom and for a
specific confidence level
• Degrees of freedom = the product of the
degrees of freedom for columns and rows
Discrete Variables

−
i E
O E 2 ( )
DF = (C −1)*(R −1)

###Entropy
    
```{r}
# Re-read the TM dataset 
TM = read.table(("C:/MOVEDELETE/OneDrive-2018-11-05/AdvancedRCode/TM.csv"),
                sep = ",", header = TRUE,
                stringsAsFactors = TRUE)
attach(TM)
View(TM)

# Education is ordered
Education = factor(Education, order = TRUE,
                   levels = c("Partial High School",
                              "High School", "Partial College",
                              "Bachelors", "Graduate Degree"))
plot(Education, main = 'Education',
     xlab = 'Education', ylab = 'Number of Cases',
     col = "purple")

# Crosstabulation with table() and xtabs()
table(Education, Gender, BikeBuyer)
table(NumberCarsOwned, BikeBuyer)
xtabs(~Education + Gender + BikeBuyer)
xtabs(~NumberCarsOwned + BikeBuyer)

# Storing tables in objects
tEduGen <- xtabs(~Education + Gender)
tNcaBik <- xtabs(~NumberCarsOwned + BikeBuyer)

# Test of independece
chisq.test(tEduGen)
chisq.test(tNcaBik)

summary(tEduGen)
summary(tNcaBik)
?xtabs
```


###Mutual Information
MI of two variables is a measure of mutual dependence
  quantifies the amount of information obtained about one variable from the other
  more general than correlation coefficient
  
###Measure the Association
Measure the association with phi coefficeint, contingency coefficient, etc...
```{r}
# Installing and loading the vcd package
 #install.packages("vcd")
library(vcd)

# Measures of association
assocstats(tEduGen)
assocstats(tNcaBik)

# Visualizing the crosstabulation
# Showing expected and observed frequencies
strucplot(tNcaBik, residuals = NULL, shade = TRUE,
          gp = gpar(fill = c("yellow", "blue")),
          type = "expected", main = "Expected")
strucplot(tNcaBik, residuals = NULL, shade = TRUE,
          gp = gpar(fill = c("yellow", "blue")),
          type = "observed", main = "Observed")


# Mutual information
 #install.packages("DescTools");
library("DescTools")
MutInf(NumberCarsOwned, Gender);
MutInf(NumberCarsOwned, HouseOwnerFlag);
MutInf(NumberCarsOwned, Age);
MutInf(NumberCarsOwned, CommuteDistance);
MutInf(NumberCarsOwned, YearlyIncome);


# Covariance and correlations

# Pearson
x <- TM[, c("YearlyIncome", "Age", "NumberCarsOwned")]
cov(x)
cor(x)

# Spearman
y <- TM[, c("TotalChildren", "NumberChildrenAtHome", "HouseOwnerFlag", "BikeBuyer")]
cor(y)
cor(y, method = "spearman")

# Two matrices correlations
cor(y, x)

# Visualizing the correlations
#install.packages("corrgram")
library(corrgram)
corrgram(y, order = TRUE, lower.panel = panel.shade,
         upper.panel = panel.shade, text.panel = panel.txt,
         cor.method = "spearman", main = "Corrgram")


# Continuous and discrete variables

# T-test
t.test(YearlyIncome ~ Gender)
t.test(YearlyIncome ~ HouseOwnerFlag)
# Error - t-test supports only two groups
t.test(YearlyIncome ~ Education)

# Visualizing the associations
boxplot(YearlyIncome ~ Gender,
        main = "Yearly Income in Groups",
        ylab = "Yearly Income",
        xlab = "Gender")
boxplot(YearlyIncome ~ HouseOwnerFlag,
        main = "Yearly Income in Groups",
        notch = TRUE,
        varwidth = TRUE,
        col = "orange",
        ylab = "Yearly Income",
        xlab = "House Owner Flag")
```

###Anova
ANOVA tests the variance in means
between groups
– Null Hypothesis: the only variance comes
from variance within and not between
samples
– Mean squared deviation between a groups,
with denoting group mean and denoting
the total mean
```{r}
# One-way ANOVA
aggregate(YearlyIncome, by = list(Education), FUN = mean)
aggregate(YearlyIncome, by = list(Education), FUN = sd)
AssocTest <- aov(YearlyIncome ~ Education)
summary(AssocTest)

# Education is ordered
Education = factor(Education, order = TRUE,
                   levels = c("Partial High School",
                              "High School", "Partial College",
                              "Bachelors", "Graduate Degree"))
# Visualizing ANOVA
boxplot(YearlyIncome ~ Education,
        main = "Yearly Income in Groups",
        notch = TRUE,
        varwidth = TRUE,
        col = "orange",
        ylab = "Yearly Income",
        xlab = "Education")

# Load gplots - another way to plot group means
library(gplots)
plotmeans(YearlyIncome ~ Education,
          bars = TRUE, p = 0.99, barwidth = 3,
          col = "red", lwd = 3,
          main = "Yearly Income in Groups",
          ylab = "Yearly Income",
          xlab = "Education")


# ggplot
# install.packages("ggplot2")
library("ggplot2")

# Plots with count (number) Education by Region
ggplot(TM, aes(Region, fill = Education)) +
  geom_bar(position = "stack")

ggplot(TM, aes(Region, fill = Education)) +
  geom_bar(position = "fill")

ggplot(TM, aes(Region, fill = Education)) +
  geom_bar(position = "dodge")


# A smaller data frame for the purpuse of graph
TMLM <- TM[1:100, c("YearlyIncome", "Age")]

# Plot the data points
plot(TMLM$Age, TMLM$YearlyIncome,
     cex = 2, col = "orange", lwd = 2)
# Plots with ggplot
# Basic
ggplot(data = TMLM, aes(x = Age, y = YearlyIncome)) +
  geom_point()

# Plot with a Lowess line
plot(TMLM$Age, TMLM$YearlyIncome,
     cex = 2, col = "orange", lwd = 2)
lines(lowess(TMLM$Age, TMLM$YearlyIncome),
      col = "blue", lwd = 2)

# With ggplot - linear + loess
ggplot(data = TMLM, aes(x = Age, y = YearlyIncome)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red") +
  geom_smooth(color = "blue")

# Education is ordered
TM$Education = factor(TM$Education, order = TRUE,
                      levels = c("Partial High School",
                              "High School", "Partial College",
                              "Bachelors", "Graduate Degree"))
# Boxplot
boxplot(TM$YearlyIncome ~ TM$Education,
        main = "Yearly Income in Groups",
        notch = TRUE,
        varwidth = TRUE,
        col = "orange",
        ylab = "Yearly Income",
        xlab = "Education")

# Boxplot with ggplot
ggplot(TM, aes(x = Education, y = YearlyIncome)) +
  geom_boxplot(fill = "orange",
               color = "blue", notch = FALSE)

# Boxplot and violin plot with ggplot
ggplot(TM, aes(x = Education, y = YearlyIncome)) +
  geom_violin(fill = "lightgreen") +
  geom_boxplot(fill = "orange",
               width = 0.2)

# Density plot
ggplot(TM, aes(x = YearlyIncome, fill = Education)) +
  geom_density(alpha = 0.3)

# Trellis charts
ggplot(TM, aes(x = NumberCarsOwned, fill = Region)) +
  geom_bar(stat = "bin") +
  facet_grid(MaritalStatus ~ BikeBuyer) +
  theme(text = element_text(size = 30))

ggplot(TM, aes(x = NumberCarsOwned, fill = Region)) +
  geom_bar(stat = "bin") +
  facet_grid(MaritalStatus ~ BikeBuyer) +
  theme(text = element_text(size = 30))

ggplot(TM, aes(x = Education, y = BikeBuyer, fill = Region)) +
  geom_bar(stat = "identity") +
  facet_grid(. ~ Region) +
  theme(legend.position = "none", axis.text.x = element_text(angle = 45))

# Jointplot
 #install.packages("ggExtra")
library(ggExtra)

plot1 <- ggplot(TMLM, aes(x=Age,y=YearlyIncome)) + 
  geom_point() +
  geom_smooth(method = "lm", color = "red") +
  geom_smooth(color = "blue")

# default: type="density"
ggMarginal(plot1, type="histogram", bins=20)
ggMarginal(plot1, type="density")

# Detach TM
detach(TM)
```


##Bayesian Inference

```{r}
# Bayesian inference
# Package e1071 
 #install.packages("e1071", dependencies = TRUE)
library(e1071)

str(TM)

# Build the Naive Bayes model
TMNB <- naiveBayes(TM[,2:11], TM[,12]) #12th column is bikebuyer

# Apriori probabilities for the target variable (BikeBuyer)
TMNB$apriori
# Apriori probabilities for the input variables in classes of the target variable
TMNB$tables

# Predicitions
predict(TMNB, TM, type = "raw")

# Data frame with predictions for all rows
df_PR <- as.data.frame(predict(
  TMNB, TM, type = "raw"))

# Combine original data with predictions
df_TM_PR <- cbind(TM, df_PR)
View(df_TM_PR)
```

###Linear Regression


``` {r{}
# Linear regression
# A smaller data frame 
TMLM <- TM[1:100, c("YearlyIncome", "Age")]
# Adding the smaller data frame to the search path
attach(TMLM)

# Plot the data points
plot(Age, YearlyIncome,
     cex = 2, col = "orange", lwd = 2)

# Simple linear regression model
LinReg1 <- lm(YearlyIncome ~ Age)
summary(LinReg1)
#Take Age coefficient * age and add that value to the intercept

# Polynomial  regression
LinReg2 <- lm(YearlyIncome ~ Age + I(Age ^ 2))
summary(LinReg2)

LinReg3 <- lm(YearlyIncome ~ Age + I(Age ^ 2) + I(Age ^ 3))
summary(LinReg3)

# Visualization
plot(Age, YearlyIncome,
     cex = 2, col = "orange", lwd = 2)
abline(LinReg1,
       col = "red", lwd = 2)
lines(lowess(Age, YearlyIncome),
      col = "blue", lwd = 2)

# Plot the first model to check the validity of the assumptions
plot(LinReg1)
# Residuals vs Fitted plot has U shape - potential linearity problem
# Normal Q-Q (quantile-quantile) exposes potential outliers
# Scale-Location shows U shape, potential heteroscedasticity
# Residuals vs Leverage - Cook's distance > 1 might show outliers problem

# package car qqPlot also shows confidence interval
# install.packages("car")
library(car)
qqPlot(LinReg1)

# can see some problematic outliers

# Check polynomial regressions
qqPlot(LinReg2)
qqPlot(LinReg3)
# The LinReg3 model looks the best

# Removing the smaller data frame from the search path
detach(TMLM)
```


###Feature Selection
Input variables or features are numerous
Collineraity might be an issue

* Drop some with high association to dependent variable
* select using advanced methods
* expect drop in quality of predictions

####Reqularization Methods
ordinal sum of squares method (OSS)
try to mimimize the residual sum of squares (RSS) - e.g. forward stepwise regression

```{r}
# Advanced R module 4

# Ridge regression
# install.packages("ElemStatLearn")  # Prostate cancer data
# install.packages("glmnet")         # Ridge and LASSO regression
# install.packages("caret")          # parameter tuning
# install.packages("caret")
library(ElemStatLearn)
library(car)
library(corrplot)
library(leaps)
library(glmnet)
library(caret)

# Check the prostate cancer data at 
# https://rdrr.io/cran/ElemStatLearn/man/prostate.html

# Load the data
data("prostate")

# Quick overview
View(prostate)
plot(prostate)
plot(prostate$gleason)
table(prostate$gleason)
# gleason values 8 and 9 could be exceptions
# Check how the gleason influences the target variable lpsa (level of prostate-specific antigen)
boxplot(prostate$lpsa~prostate$gleason)
# Can recode together with 7
prostate$gleason <- ifelse(prostate$gleason == 6, 0, 1)
table(prostate$gleason)

# Check the collinearity
corm <- cor(prostate)
corm
corrplot.mixed(corm)
# Some high correlations

# Define train and test sets (last variable in the dataset)
prostateTrain <- subset(prostate, train == TRUE)[, 1:9]
prostateTest <- subset(prostate, train == FALSE)[, 1:9]

# Best subsets
bestsubFit <- regsubsets(lpsa~., data=prostateTrain)
bsf.summ <- summary(bestsubFit)
which.min(bsf.summ$bic)
# Model with 3 features has the lowest
# Bayesian Information Criterion BIC value
plot(bsf.summ$bic, type = "l", xlab = "Features N", ylab = "BIC")
plot(bestsubFit, scale = "bic", main = "Best Subset")
# Features to use: lcavol, lweight, gleason

# Use OLS
olsFit <- lm(lpsa~lcavol+lweight+gleason, data = prostateTrain)
# Plot predicted vs actual
plot(olsFit$fitted.values, prostateTrain$lpsa, xlab = "Predicted", ylab = "Actual")
# Check with the test set
olsFitPred <- predict(olsFit, newdata = prostateTest)
plot(olsFitPred, prostateTest$lpsa, xlab = "Predicted", ylab = "Actual")
# Calculate the mean squared error
residOls <- prostateTest$lpsa - olsFitPred
mean(residOls^2)
# 0.5084126


# Ridge regression
# glmnet function needs matrix inputs
X <- as.matrix(prostateTrain[,1:8])
y <- prostateTrain[,9]
ridgeFit <- glmnet(X, y, family = "gaussian", alpha = 0)
print(ridgeFit)
# coefficient values vs L1-norm
# top x axis - number of coefficients (constant 8)
plot(ridgeFit, label = TRUE)
# plot changes of coefficients with λ
plot(ridgeFit, xvar = "lambda", label = TRUE)

# Show coefficient at specific λ
ridgeFit.coef <- coef(ridgeFit, s=0.1, exact = TRUE)
ridgeFit.coef
# Some are close to zero

# Plot deviance vs coefficients
plot(ridgeFit, xvar="dev", label = TRUE)

# Use λ = 0.1 for prediction
X.R <- as.matrix(prostateTest[,1:8])
y.R <- prostateTest[,9]

ridgeFitPred <- predict(ridgeFit, newx = X.R, type = "response", s = 0.1)
residRidge <- prostateTest$lpsa - ridgeFitPred
mean(residRidge^2)
# 0.4783559 - better thank OLS

```

###Matrix Operations
Linear algebra

* Principal Component Analysis
    + extracts eigenvectors (direction of the line) and eigenvalues (how much variance there is in the data in that direction - spread)
    + lowers the dimensionality
    + eigenvaector with the highest eigenvalue is the principal component
    
* Rotations of principal components
    + rotate axes of multidimensional space
    + maximize association of PCs with different subsets of variable inputs
    + can be orthogonal to where PCs are still uncorrelated (use this in PCA), or oblique, where correlated PCs is allowed
    
* Exploratory Factor Analysis
  The factors are the underlying combined variables that help you understand your data
  
  
```{r PCA}
# Basic matrix operations
# Vectors
rep(1,10)                 # replicate a number n times
seq(3,7)                  # Sequence of integers between 3 and 7
3:7                       # Equivalent sequence
seq(5,17,by=3)            # Every 3rd integer between 5 and 17
# Vector variables
x <- c(2,0,0,4)           # A vector with elements 2,0,0,4
assign("y", c(1,9,9,9))   # Another way to assign a vector
c(5,4,3,2) -> z           # Another way to assign a vector      
q = c(1,2,3,4)            # Another way to assign a vector
# Vector operations
x + y                     # Sums elements of two vectors
x * 4                     # Multiplies elements
sqrt(x)                   # Function applies to each element
# Vector elements
x <- c(2,0,0,4)
x[1]               # Select the first element
x[-1]              # Exclude the first element
x[1] <- 3; x       # Assign a value to the first element
x[-1] = 5; x       # Assign a value to all other elements
y <- c(1,9,9,9)
y < 8              # Compares each element, returns result as vector
y[4] = 1
y < 8
y[y<8] = 2; y      # Edits elements marked as TRUE in index vector

# Matrices, Arrays, factors, lists, data frames
# Array
x = c(1,2,3,4,5,6); x         # A simple vector
Y = array(x, dim=c(2,3)); Y   # A matrix from the vector - fill by columns
Z = matrix(x,2,3,byrow=F); Z  # A matrix from the vector - fill by columns
U = matrix(x,2,3,byrow=T); U  # A matrix from the vector - fill by rows

# Matrix operations
U <- matrix(c(1,2,3,4), 2, 2, byrow=T)
V <- matrix(c(5,6,7,8), 2, 2, byrow=T)
A <- matrix(c(1,2,3,4,5,6), 2, 3, byrow=T)
B <- matrix(c(1,2,3,4,5,6), 3, 2, byrow=T);

U; V
A; B

U + V; V + U        # Addition
V - U; U - V        # Subtraction
U %*% V; V %*% U    # Multiplication

U %*% A             # Conformable
U %*% B             # Non-comformable

cbind(U, V)         # Combine by columns
rbind(V, U)         # Combine by rows

cbind(U, A)         # Combine by columns
cbind(U, B)         # Combine by columns - error, number of rows must match
rbind(U, A)         # Combine by rows - error, number of columns must match
rbind(U, B)         # Combine by rows

# Determinants
U; det(U)    # -2
V; det(V)    # -2
A; det(A)    # Error
B; det(B)    # Error

# 3x3 example
C <- matrix(c(1, -2, 2, 2, 0, 1, 1, 1, -2), 3, 3, byrow=T)
C
det(C)       # -7

# Eigenvectors
D <- matrix(c(1, 4, 9, 1), 2, 2, byrow = T)
D
det(D)
eigen(D)

E <- matrix(c(2, 1, 1, 1, 2, 1, 1, 1, 2), 3, 3, byrow=T)
E
det(E)
eigen(E)


# Principal component analysis (PCA)
# Reading a data frame from a CSV file
TM = read.table("C:\\AdvancedR\\TM.csv",
                sep = ",", header = TRUE,
                stringsAsFactors = TRUE)
library(tidyverse)
TM <- read_csv("C:/MOVEDELETE/OneDrive-2018-11-05/AdvancedRCode/TM.csv")

# Extracting numerical data only
TMPCAEFA <- TM[, c("TotalChildren", "NumberChildrenAtHome",
                   "HouseOwnerFlag", "NumberCarsOwned",
                   "BikeBuyer", "YearlyIncome", "Age")]

# PCA from the base installation
pcaBasic <- princomp(TMPCAEFA, cor = TRUE)
summary(pcaBasic)
plot(pcaBasic, main = "PCA Basic", col = "blue")


# Package psych functions used for PCA and EFA
# install.packages("psych")
library(psych)
#install.packages("psych")
# PCA unrotated
pcaTM_unrotated <- principal(TMPCAEFA, nfactors = 2, rotate = "none")
pcaTM_unrotated

# PCA varimax rotation
pcaTM_varimax <- principal(TMPCAEFA, nfactors = 2, rotate = "varimax")
pcaTM_varimax

# Biplots
biplot.psych(pcaTM_unrotated, cex = c(0.1, 2), main = "PCA Unrotated")
biplot.psych(pcaTM_varimax, cex = c(0.1, 2), main = "PCA Varimax")


# EFA unrotated
efaTM_unrotated <- fa(TMPCAEFA, nfactors = 2, rotate = "none")
efaTM_unrotated

# EFA varimax
efaTM_varimax <- fa(TMPCAEFA, nfactors = 2, rotate = "varimax")
efaTM_varimax

# EFA promax
efaTM_promax <- fa(TMPCAEFA, nfactors = 2, rotate = "promax")
efaTM_promax
#install.packages("GPArotation")
# Plots
factor.plot(efaTM_unrotated,
            labels = rownames(efaTM_unrotated$loadings),
            title = "EFA Unrotated")
factor.plot(efaTM_varimax,
            labels = rownames(efaTM_varimax$loadings),
            title = "EFA Varimax")
factor.plot(efaTM_promax,
            labels = rownames(efaTM_promax$loadings),
            title = "EFA Promax")
fa.diagram(efaTM_unrotated, simple = FALSE,
           main = "EFA Unrotated")
fa.diagram(efaTM_varimax, simple = FALSE,
           main = "EFA Varimax")
fa.diagram(efaTM_promax, simple = FALSE,
           main = "EFA Promax")
```


    

```{r Princpal Component Analysis}


```


